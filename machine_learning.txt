#machine learning
##机器学习的目的:
    机器学习可以揭示数据背后的真实含义

##数据挖掘十大算法
    1. C4.5决策树
    2. k-均值(k-mean)
    3. 支持向量机(SVM)
    4. Apriori
    5. 最大期望算法(EM)
    6. PageRank算法
    7. AdaBoost算法
    8. k-近邻算法(kNN)
    9. 朴素贝叶斯算法(NB)
    10. 分类回归树(CART)算法

##机器学习的主要任务
    1. 监督学习
        1). 解决分类问题
            主要是将实例数据划分到合适的分类中
        2). 回归问题
            主要用于预测数值型数据
    2. 无监督
        1). 聚类
            将数据集合分成由相似的对象组成的多个类的过程被成为聚类
        2). 密度估计
            将寻找描述数据统计值的过程称为密度估计

##开发机器学习应用程序的步骤
    1. 收集数据
    2. 准备输入数据
    3. 分析输入数据
    4. 训练算法
    5. 测试算法
    6. 使用算法

##朴素贝叶斯分类
    朴素的含义:
        只做最简单的假设
            某些单词会搭配出现，但是朴素贝叶斯不做这样的假设，认为特征单词相互独立
            每个单词同等重要
    优点:
        在数据较少的情况下依然有效，可以处理多类别问题

    缺点:
        对于输入数据的准备方式较为敏感  什么叫准备方式？
        
    适用范围:
        标称型数据
    
    核心思想:
        使用p1(x,y)来表示数据点(x,y)是属于p1类别的概率
        使用p2(x,y)来表示数据点(x,y)是属于p2类别的概率
        对于一个新的点(x1,y1), 如果p1(x1,y1) > p2(x1,y1)，那么(x1, y1)属于类别p1, 
        反之如果p2(x1,y1)>p1(x1,y1), 那么属于p2
        
        p(c|x) = p(c and x) / p(x)
        在x条件下, c发生的概率等于在c and x同时成立的概率除以x发生的概率
        p(c|x) * p(x) = p(c and x) 这个等式稍微好理解一点，相当于说乘以x发生的概率p(x), 
        把x条件坐实
        
        如果p(c1|x,y) > p(c2|x,y), 那么属于类别1
        如果p(c1|x,y) < p(c2|x,y), 那么属于类别2
        p(ci|x,y) = p(x,y|ci)p(ci)/p(x,y)  ==> p(ci|x,y) * p(x,y) = p(x,y|ci)

##Logistic回归
    假设一些数据点，用一条直线对这些点进行拟合(该直线成为最佳拟合直线), 这个拟合过程就叫做回归.
    Logistic回归
    优点:
        计算代价不高哦， 易于理解和实现
    缺点:
        容易欠拟合，分类精度可能不高
    适用数据类型:
        数值型和标称型数据
    使用Heaviside step function(海维塞德阶跃函数), 简化可使用Sigmoid函数进行分类:
        z = 1/(1+e^(-z))
    输入记作:
        z = w0*x0 + w1*x1 + w2*x2 ... + wn*xn
    其中向量x是分类器的输入数据, w是要找到的最佳参数, 从而使得分类器尽可能的精确.
    
    梯度上升和梯度下降
        假设有函数f(x,y), 则在某点的梯度就是对x和y的分别偏导函数. 
        它们意味着找一个函数f(x,y)的最大值或者最小值,需要分别沿着x和y的方向进行移动，
        移动量就是偏导函数计算出来的量.
        在移动处理过程之中，移动步长就是我们要训练和确定的量，记作a.
        最终梯度算法迭代公式如下:
        w:=w+a * f(w)的偏导函数 
        其中w是特征向量, (+号视情况变为-号，即下降)
    
##AdaBoost
    做重要决定的时候，考虑吸收多个专家而不是一个人的意见，这就是元算法(meta-algorithm)的思路.
    优点:
        泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整
    缺点:
        对离群点敏感
    使用数据类型:
        数值型和标称型数据
    
    核心思想:
        adaptive boosting(自适应boostring)的缩写, 具体方法:
        1. 给训练数据中的每一个样本赋予都有相同值的权重Di
        2. 给每个分类器都分配一个权重值alpha
        3. 定义错误率 xigema = 未正确分类的样本数目/所有样本数目
            alpha = 1/2 * ln((1-xigema)/xigema)
        4. 每一次训练都会对样本权重和分类器权重进行调整
            如果该样本分类正确
            Di := Di * e^(-alpha) / sum(D)          降低了权值
            如果分类错误
            Di := Di * e^(alpha) / sum(D)
        5. 直到训练错误率xigema为0或弱分类器数目达到我们指定的值为止
            (能够较好的完成分类任务的分类器叫做强分类器，而比随机分类器效果好，
            但是好不到哪里的分类器就称之为弱分类器.在adaboost中的分类器中就可能存在弱分类器)

#k-means algorithm
    优点:
        易于实现
    缺点:
        可能收敛到局部最小值，在大规模数据集上收敛较慢
    适用数据类型:
        数值型数据
    
    核心思想：
        1. 随机确定k个初始点作为质心
        2. 然后将数据集中的每个点分配到一个簇中
        3. 为每个点找距其最近的质心，并将该质心所对应的簇
        4. 每个簇的质心更新为该簇的所有点的平均值
    
    bisecting K-means二分K-均值
    k-means algorithm 收敛于局部最小值，而非全局最小值, 为了克服这个问题，可以使用
    二分k-均值(bisecting K-means)的算法:
        将所有点都看成一个簇
        当簇的数目小于k时
        对于每一个簇
            计算总误差
            在给定的簇上面进行k-均值聚类(k=2)
            计算将该簇一分为二之后的总误差
        选择使的误差最小的那个簇进行划分操作
    
#关联分析 Apriori
    从大规模数据之中寻找物品间的隐藏关系被称作关联分析(association analysis)
    或者关联规则学习(association rule learning)
    
